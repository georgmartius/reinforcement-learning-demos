<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <title>REINFORCE / Policy Gradient with MC on CartPole (3 actions)</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.20.0/dist/tf.min.js"></script>
    <style>
     body { font: 14px/1.3 system-ui, -apple-system, Segoe UI, Roboto, Arial, sans-serif; margin: 16px; }
     .row { display: flex; gap: 16px; align-items: flex-start; flex-wrap: wrap; }
     .panel { border: 1px solid #ddd; border-radius: 8px; padding: 12px; }
     #canvas { background: #f9fafb; border: 1px solid #ddd; border-radius: 8px; }
     .controls label { display:block; margin: 6px 0 2px; }
     .controls input[type="number"], .controls select { width: 120px; }
     .grid { display: grid; grid-template-columns: auto auto; gap: 6px 12px; align-items: center; }
     .stat { min-width: 180px; }
     .bar { height: 10px; background: #ddd; width: 120px; border-radius: 4px; overflow: hidden; }
     .fill { height: 10px; background: #4f46e5; }
     .note { color: #555; }
     button { padding: 6px 12px; }
     #chart { background: #fff; border: 1px solid #ddd; border-radius: 8px; }
    </style>
  </head>
  <body>
    <h2>REINFORCE / Policy Gradient with MC on CartPole (3 actions: Left, None, Right)</h2>
    <div class="row">
      <canvas id="canvas" width="640" height="220"></canvas>

      <div class="panel">
        <div class="stat">Episodes (trained): <span id="episodes">0</span></div>
        <div class="stat">Last episode return: <span id="lastReturn">0</span></div>
        <div class="stat">Avg return (100 ep): <span id="avgReturn">0</span></div>
        <div class="stat">Last ep length: <span id="lastLen">0</span></div>
        <div class="stat">Policy entropy (preview s): <span id="entropyStat">0</span></div>
        <hr/>
        <div>Action probabilities (preview state)</div>
        <div>Left</div><div class="bar"><div id="p0" class="fill" style="width:0%;"></div></div>
        <div>None</div><div class="bar"><div id="p1" class="fill" style="width:0%;"></div></div>
        <div>Right</div><div class="bar"><div id="p2" class="fill" style="width:0%;"></div></div>
      </div>
      <div class="panel controls">

        <div class="grid">
          <label>Learning rate</label>
          <input id="lr" type="number" step="0.001" value="0.01" />
          <label>Gamma (γ)</label>
          <input id="gamma" type="number" step="0.01" value="0.99" />
          <label>Entropy coeff (β)</label>
          <input id="entropy" type="number" step="0.001" value="0.0" />
          <label>Baseline</label>
          <select id="baseline">
            <option value="none" selected>None</option>
            <option value="moving">Moving average (scalar)</option>
            <option value="value">Learned V(s) network</option>
          </select>
          <label>Return</label>
          <select id="rtg">
            <option value="true" selected>Return G (recomm.)</option>
            <option value="false">Total Episode Return</option>
          </select>
          <label>Normalize advantages/returns</label>
          <select id="normAdv">
            <option value="true" selected>On</option>
            <option value="false">Off</option>
          </select>
          <label>Batch episodes / update</label>
          <input id="batchSize" type="number" step="1" value="5" />
          <label>Max steps per episode</label>
          <input id="maxSteps" type="number" step="10" value="500" />
          <label>Preview after each update</label>
          <input id="doPreview" type="checkbox" checked />
          <label>Manual step delay (ms)</label>
          <input id="stepDelay" type="number" step="20" value="40" />
        </div>
        <div style="margin-top:10px;">
          <button id="startBtn">Train</button>
          <button id="pauseBtn">Pause training</button>
          <button id="stepBtn">Test 1 episode</button>
          <button id="resetBtn">Reset</button>
        </div>
        <p class="note">REINFORCE: baseline=None, β=0.0.</p>
      </div>

    </div>

    <div class="panel" style="margin-top:12px;">
      <div style="margin-bottom:6px;">Training progress</div>
      <canvas id="chart" width="960" height="240"></canvas>
    </div>

    <script>
     // =============== Environment: CartPole (3 actions) ==================
     // 0: left (-F), 1: none (0), 2: right (+F)
     class CartPole {
       constructor(seed=0) {
         this.gravity = 9.8;
         this.masscart = 1.0;
         this.masspole = 0.1;
         this.total_mass = this.masscart + this.masspole;
         this.length = 0.5; // half the pole's length
         this.polemass_length = this.masspole * this.length;
         this.force_mag = 10.0;
         this.tau = 0.02; // seconds between state updates
         this.theta_threshold_radians = 12 * Math.PI / 180;
         this.x_threshold = 2.4;
         this.np = this._rng(seed);
         this.reset();
       }
       _rng(seed) {
         let s = Math.imul(seed ^ 0x9e3779b9, 0x85ebca6b) >>> 0;
         return () => {
           s ^= s << 13; s ^= s >>> 17; s ^= s << 5; s >>>= 0;
           return (s & 0xfffffff) / 0xfffffff;
         };
       }
       reset() {
         this.state = [
           (this.np()-0.5)*0.1, // x
           (this.np()-0.5)*0.1, // x_dot
           (this.np()-0.5)*0.2, // theta
           (this.np()-0.5)*0.1  // theta_dot
         ];
         this.steps = 0;
         return this.state.slice();
       }
       step(action) {
         let [x, x_dot, theta, theta_dot] = this.state;
         const force = (action === 0 ? -this.force_mag : (action === 2 ? this.force_mag : 0.0));
         const costheta = Math.cos(theta), sintheta = Math.sin(theta);

         const temp = (force + this.polemass_length * theta_dot * theta_dot * sintheta) / this.total_mass;
         const thetaacc = (this.gravity * sintheta - costheta * temp) / (this.length * (4.0/3.0 - this.masspole * costheta * costheta / this.total_mass));
         const xacc = temp - this.polemass_length * thetaacc * costheta / this.total_mass;

         // Euler integration
         x += this.tau * x_dot;
         x_dot += this.tau * xacc;
         theta += this.tau * theta_dot;
         theta_dot += this.tau * thetaacc;

         this.state = [x, x_dot, theta, theta_dot];
         this.steps += 1;

         const done =
         x < -this.x_threshold || x > this.x_threshold ||
         theta < -this.theta_threshold_radians || theta > this.theta_threshold_radians;

         const reward = done ? 0 : 1; // +1 per step alive
         return {state: this.state.slice(), reward, done};
       }
     }

     // =============== Model: Policy (and optional Value) =================
     const obsDim = 4, nActions = 3;

     function buildPolicy(hidden=16) {
       const model = tf.sequential();
       model.add(tf.layers.dense({units: hidden, inputShape: [obsDim], activation: 'tanh', kernelInitializer: 'glorotUniform'}));
       model.add(tf.layers.dense({units: nActions, activation: 'linear', kernelInitializer: 'glorotUniform'}));
       return model;
     }
     function buildValue(hidden=32) {
       const model = tf.sequential();
       model.add(tf.layers.dense({units: hidden, inputShape: [obsDim], activation: 'tanh', kernelInitializer: 'glorotUniform'}));
       model.add(tf.layers.dense({units: 1, activation: 'linear', kernelInitializer: 'glorotUniform'}));
       return model;
     }

     let policy = buildPolicy(16);
     let valueNet = buildValue(32);

     // =============== Training utilities ================================
     function categoricalSample(probs) {
       const p = probs.dataSync(); // nActions
       const r = Math.random();
       let cum = 0, a = 0;
       for (let i=0;i<p.length;i++){ cum += p[i]; if (r <= cum) { a = i; break; } }
       return a;
     }
     function entropyOfProbs(probs) {
       const p = probs.dataSync();
       let h = 0.0;
       for (let i=0;i<p.length;i++) if (p[i] > 1e-12) h -= p[i] * Math.log(p[i] + 1e-12);
       return h;
     }
     function computeReturns(rewards, gamma, rewardToGo=true) {
       const T = rewards.length;
       const G = new Array(T);
       if (rewardToGo) {
         let running = 0;
         for (let t=T-1; t>=0; --t) {
           running = rewards[t] + gamma * running;
           G[t] = running;
         }
       } else { // like evolutionary search, only global return.
         let total = 0;
         for (let t=T-1; t>=0; --t) total = rewards[t] + gamma * total;
         for (let t=0; t<T; ++t) G[t] = total;
       }
       return G;
     }
     function zscore(arr) {
       const m = arr.reduce((a,b)=>a+b,0)/arr.length;
       const v = arr.reduce((a,b)=>a+(b-m)*(b-m),0)/arr.length + 1e-8;
       const s = Math.sqrt(v);
       return arr.map(x => (x - m)/s);
     }

     // =============== Replay and optimization ===========================
     let env = new CartPole(0);
     let policyOpt = tf.train.adam(0.01);
     let valueOpt  = tf.train.adam(0.01);

     let training = false;
     let trainedEpisodes = 0;
     let returnsHistory = [];
     let movingBaseline = 0;
     let emaAlpha = 0.01;

     async function runEpisode(env, opts, render=false, renderDelayMs=0) {
       const sList = [], aList = [], rList = [];
       let done = false, totalR = 0, steps = 0;
       let state = env.reset();

       while (!done && steps < opts.maxSteps) {
         const logits = tf.tidy(() => policy.predict(tf.tensor2d([state])));
         const probs = tf.tidy(() => tf.softmax(logits));
         const action = categoricalSample(probs);

         sList.push(state.slice());
         aList.push(action);

         const {state: nextState, reward, done: doneFlag} = env.step(action);
         rList.push(reward);
         totalR += reward;
         state = nextState;
         done = doneFlag;
         steps++;

         if (render && steps % 2 === 0) {
           updateProbsUI(probs);
           drawEnv(env);
           // slow down visualization if requested (used by manual Step)
           if (renderDelayMs > 0) {
             await new Promise(r => setTimeout(r, renderDelayMs));
           }
           await tf.nextFrame();
         }
         logits.dispose(); probs.dispose();
       }
       return {sList, aList, rList, totalR, steps};
     }

     async function trainBatch(opts) {
       const batch = [];
       const epReturns = [];
       for (let i=0;i<opts.batchSize;i++) {
         const ep = await runEpisode(env, opts, false);
         batch.push(ep);
         epReturns.push(ep.totalR);
         trainedEpisodes++;
         updateStats(ep.totalR, ep.steps);
       }

       // Flatten batch
       const states = [];
       const actions = [];
       const returns = [];
       for (const ep of batch) {
         const G = computeReturns(ep.rList, opts.gamma, opts.rewardToGo);
         for (let t=0; t<ep.sList.length; t++) {
           states.push(ep.sList[t]);
           actions.push(ep.aList[t]);
           returns.push(G[t]);
         }
       }

       const S = tf.tensor2d(states);                // [N,4]
       const A = tf.tensor1d(actions, 'int32');      // [N]
       const G = tf.tensor1d(returns, 'float32');    // [N]

       // Baseline
       let advantages;
       const baselineType = document.getElementById('baseline').value;
       if (baselineType === 'moving') {
         const meanG = returns.reduce((a,b)=>a+b,0)/returns.length;
         movingBaseline = (1-emaAlpha)*movingBaseline + emaAlpha*meanG;
         advantages = tf.tidy(()=> tf.sub(G, tf.scalar(movingBaseline)));
       } else if (baselineType === 'value') {
         await valueOpt.minimize(() => {
           const vPred = valueNet.predict(S).reshape([S.shape[0]]);
           return tf.mean(tf.square(tf.sub(G, vPred)));
         }, true, valueNet.trainableWeights.map(w=>w.val));
         const vPred = valueNet.predict(S).reshape([S.shape[0]]);
         advantages = tf.sub(G, vPred);
         vPred.dispose();
       } else {
         advantages = G.clone();
       }

       // Optionally normalize advantages
       if (opts.normalizeAdvantages) {
         const advArr = await advantages.data();
         advantages.dispose();
         advantages = tf.tensor1d(zscore(Array.from(advArr)), 'float32');
       }

       const beta = parseFloat(document.getElementById('entropy').value) || 0.0;

       // Policy update via REINFORCE
       await policyOpt.minimize(() => {
         const logits = policy.predict(S);                 // [N,3]
         const logProbs = tf.logSoftmax(logits);          // [N,3]
         const oneHotA = tf.oneHot(A, nActions).toFloat();// [N,3]
         const logpA = tf.sum(tf.mul(oneHotA, logProbs), 1);  // [N]
         let loss = tf.neg(tf.mean(tf.mul(logpA, advantages)));

         const beta = parseFloat(document.getElementById('entropy').value) || 0.0;
         if (beta > 0) {
           const probs = tf.softmax(logits);
           const ent = tf.neg(tf.sum(tf.mul(probs, tf.log(probs.add(1e-12))), 1));
           const entLoss = tf.neg(tf.mean(ent).mul(beta));
           loss = loss.add(entLoss);
         }
         return loss;
       }, true, policy.trainableWeights.map(w => w.val));

       S.dispose(); A.dispose(); G.dispose(); advantages.dispose();
     }

     function updateStats(ret, len) {
       returnsHistory.push(ret);
       if (returnsHistory.length > 1000) returnsHistory.shift();
       document.getElementById('episodes').textContent = String(trainedEpisodes);
       document.getElementById('lastReturn').textContent = ret.toFixed(0);
       const avg = returnsHistory.slice(-100).reduce((a,b)=>a+b,0)/(Math.min(returnsHistory.length,100)||1);
       document.getElementById('avgReturn').textContent = avg.toFixed(1);
       document.getElementById('lastLen').textContent = String(len);
       drawChart();
     }

     function updateProbsUI(probsTensor) {
       const p = probsTensor.dataSync();
       for (let i=0;i<3;i++) {
         const el = document.getElementById('p'+i);
         el.style.width = (100 * p[i]).toFixed(0) + '%';
       }
       const ent = entropyOfProbs(probsTensor);
       document.getElementById('entropyStat').textContent = ent.toFixed(3);
     }

     // =============== Rendering (env and chart) =========================
     const canvas = document.getElementById('canvas');
     const ctx = canvas.getContext('2d');

     function drawEnv(env) {
       const [x, , theta, ] = env.state;
       ctx.clearRect(0,0,canvas.width, canvas.height);

       // Ground
       ctx.strokeStyle = '#999'; ctx.lineWidth = 2;
       ctx.beginPath(); ctx.moveTo(20, 180); ctx.lineTo(620, 180); ctx.stroke();

       // Scale x from [-x_threshold, x_threshold] to [60, 580]
       const xpix = 60 + (x + env.x_threshold) * (520 / (2*env.x_threshold));
       const cartW = 50, cartH = 30;

       // Cart
       ctx.fillStyle = '#374151';
       ctx.fillRect(xpix - cartW/2, 150, cartW, cartH);

       // Pole
       const poleLenPix = env.length * 200;
       const cx = xpix, cy = 150;
       const px = cx + poleLenPix * Math.sin(theta);
       const py = cy - poleLenPix * Math.cos(theta);

       ctx.strokeStyle = '#f59e0b'; ctx.lineWidth = 6;
       ctx.beginPath(); ctx.moveTo(cx, cy); ctx.lineTo(px, py); ctx.stroke();

       // Axle
       ctx.fillStyle = '#111827';
       ctx.beginPath(); ctx.arc(cx, cy, 5, 0, Math.PI*2); ctx.fill();
     }

     // Simple rolling chart
     const chart = document.getElementById('chart');
     const cctx = chart.getContext('2d');
     function drawChart() {
       const w = chart.width, h = chart.height;
       cctx.clearRect(0,0,w,h);

       // axes
       cctx.strokeStyle = '#e5e7eb';
       cctx.lineWidth = 1;
       cctx.beginPath(); cctx.moveTo(40, 10); cctx.lineTo(40, h-30); cctx.lineTo(w-10, h-30); cctx.stroke();

       if (returnsHistory.length === 0) return;

       const data = returnsHistory;
       const N = data.length;
       const maxY = Math.max(50, Math.max(...data));
       const minY = 0;

       // y ticks
       cctx.fillStyle = '#6b7280';
       cctx.font = '12px system-ui';
       for (let y=0; y<=maxY; y+=Math.max(10, Math.floor(maxY/5))) {
         const py = mapY(y, minY, maxY, h);
         cctx.fillText(String(y), 8, py+4);
         cctx.strokeStyle = '#f3f4f6';
         cctx.beginPath(); cctx.moveTo(40, py); cctx.lineTo(w-10, py); cctx.stroke();
       }

       // visible window (last ~900 episodes)
       const startXIdx = Math.max(0, N - 900);
       const visible = data.slice(startXIdx);
       const firstEpisodeNumber = startXIdx + 1;     // 1-based episode index for labels
       const lastEpisodeNumber = N;

       // x ticks (episode indices)
       const tickCountTarget = 8;
       const visibleCount = visible.length;
       const rawStep = Math.max(1, Math.ceil(visibleCount / tickCountTarget));
       const step = niceStep(rawStep); // round to 1, 2, 5, 10, 20, 50, 100, ...

       cctx.fillStyle = '#6b7280';
       cctx.textAlign = 'center';
       cctx.textBaseline = 'top';
       for (let i = 0; i < visibleCount; i += step) {
         const episodeNum = firstEpisodeNumber + i;
         const x = mapX(i, 0, visibleCount-1, w);
         // tick
         cctx.strokeStyle = '#e5e7eb';
         cctx.beginPath(); cctx.moveTo(x, h-30); cctx.lineTo(x, h-26); cctx.stroke();
         // label
         cctx.fillText(String(episodeNum), x, h-24);
       }
       // also label the last episode if it wasn't aligned to a tick
       const lastX = mapX(visibleCount-1, 0, visibleCount-1, w);
       cctx.fillText(String(lastEpisodeNumber), lastX, h-24);

       // returns line
       cctx.strokeStyle = '#3b82f6';
       cctx.lineWidth = 1.5;
       cctx.beginPath();
       visible.forEach((v, i) => {
         const x = mapX(i, 0, visibleCount-1, w);
         const y = mapY(v, minY, maxY, h);
         if (i===0) cctx.moveTo(x,y); else cctx.lineTo(x,y);
       });
       cctx.stroke();

       // moving average (100)
       const ma = movingAverage(data, 100).slice(startXIdx);
       cctx.strokeStyle = '#f59e0b';
       cctx.lineWidth = 2;
       cctx.beginPath();
       ma.forEach((v, i) => {
         const x = mapX(i, 0, ma.length-1, w);
         const y = mapY(v, minY, maxY, h);
         if (i===0) cctx.moveTo(x,y); else cctx.lineTo(x,y);
       });
       cctx.stroke();

       // axis labels / legend
       cctx.textAlign = 'left';
       cctx.textBaseline = 'alphabetic';
       cctx.fillStyle = '#111827';
       cctx.fillText('Return', 100, 18);
       cctx.fillStyle = '#3b82f6';
       cctx.fillRect(70, 10, 20, 4);
       cctx.fillStyle = '#111827';
       cctx.fillText('moving average return (100)', 240, 18);
       cctx.fillStyle = '#f59e0b';
       cctx.fillRect(200, 10, 20, 4);

       // x-axis label
       cctx.fillStyle = '#111827';
       cctx.textAlign = 'center';
       cctx.textBaseline = 'bottom';
       cctx.fillText('Episode', (w-10 + 40)/2, h-2);

       function mapX(i, i0, i1, width) {
         const left = 40, right = width-10;
         if (i1 <= i0) return left;
         return left + (right-left) * (i - i0) / (i1 - i0);
       }
       function mapY(v, vmin, vmax, height) {
         const top = 10, bottom = height-30;
         const t = (v - vmin) / Math.max(1e-6, (vmax - vmin));
         return bottom - t * (bottom - top);
       }
       function niceStep(s) {
         // rounds s to a "nice" step (1, 2, 5 × 10^k)
         const pow10 = Math.pow(10, Math.floor(Math.log10(s)));
         const scaled = s / pow10;
         let nice;
         if (scaled <= 1) nice = 1;
         else if (scaled <= 2) nice = 2;
         else if (scaled <= 5) nice = 5;
         else nice = 10;
         return nice * pow10;
       }
     }

     function movingAverage(arr, k) {
       const out = [];
       let s = 0;
       for (let i=0;i<arr.length;i++) {
         s += arr[i];
         if (i >= k) s -= arr[i-k];
         out.push(s / Math.min(k, i+1));
       }
       return out;
     }

     // =============== Control loop ======================================
     let animHandle = null;

     function getOpts() {
       return {
         gamma: parseFloat(document.getElementById('gamma').value),
         lr: parseFloat(document.getElementById('lr').value),
         beta: parseFloat(document.getElementById('entropy').value),
         rewardToGo: document.getElementById('rtg').value === 'true',
         normalizeAdvantages: document.getElementById('normAdv').value === 'true',
         batchSize: parseInt(document.getElementById('batchSize').value, 10),
         maxSteps: parseInt(document.getElementById('maxSteps').value, 10)
       };
     }


     // Continuous training loop: runs until training is set to false
     async function trainingLoop() {
       // initialize optimizer with current LR
       let lastLr = parseFloat(document.getElementById('lr').value);
       policyOpt = tf.train.adam(lastLr);
       valueOpt  = tf.train.adam(lastLr);

       while (training) {
         // read current options each cycle (so changes in the UI take effect)
         const opts = getOpts();

         // if LR changed, recreate optimizer
         if (opts.lr !== lastLr) {
           lastLr = opts.lr;
           policyOpt.dispose?.();  // safe if present
           valueOpt.dispose?.();
           policyOpt = tf.train.adam(lastLr);
           valueOpt  = tf.train.adam(lastLr);
           console.log('[trainingLoop] LR changed ->', lastLr);
         }

         // 1) Train on a batch of episodes (no rendering)
         await trainBatch(opts);

         // 2) Preview one rendered episode (not counted toward training stats)
         if (document.getElementById('doPreview').checked) {
           await runEpisode(env, opts, true /*render*/);
           drawEnv(env);
         }
         // yield to UI thread so charts/controls update
         await tf.nextFrame();
       }
     }

     // Buttons
     document.getElementById('startBtn').onclick = () => {
       if (!training) { training = true; trainingLoop(); }
     };
     document.getElementById('pauseBtn').onclick = () => {
       training = false; // loop will exit after the current cycle
     };

     //  document.getElementById('startBtn').onclick = () => { if (!training) { training = true; trainingLoop(); } };
     // document.getElementById('pauseBtn').onclick = () => { training = false; if (animHandle) cancelAnimationFrame(animHandle); };
     document.getElementById('stepBtn').onclick = async () => {
       const opts = getOpts();
       const delay = parseInt(document.getElementById('stepDelay').value, 10) || 0;
       const ep = await runEpisode(env, opts, true, delay);
       document.getElementById('lastReturn').textContent = ep.totalR.toFixed(0);
       document.getElementById('lastLen').textContent = String(ep.steps);
       drawEnv(env);
     };
     document.getElementById('resetBtn').onclick = () => {
       training = false; if (animHandle) cancelAnimationFrame(animHandle);
       // dispose existing models to free GPU/CPU tensors
       try { policy.dispose(); } catch(e) {}
       try { valueNet.dispose(); } catch(e) {}

       // recreate fresh models (randomly initialized)
       policy = buildPolicy(16);
       valueNet = buildValue(32);

       env = new CartPole(Math.floor(Math.random()*1e6));
       returnsHistory = []; trainedEpisodes = 0; movingBaseline = 0;

       const lrNow = parseFloat(document.getElementById('lr').value);
       policyOpt.dispose?.();
       valueOpt.dispose?.();
       policyOpt = tf.train.adam(lrNow);
       valueOpt  = tf.train.adam(lrNow);
       drawEnv(env); drawChart();
       document.getElementById('episodes').textContent = '0';
       document.getElementById('lastReturn').textContent = '0';
       document.getElementById('avgReturn').textContent = '0';
       document.getElementById('lastLen').textContent = '0';
       for (let i=0;i<3;i++) document.getElementById('p'+i).style.width = '0%';
     };

     // Initial draw
     drawEnv(env); drawChart();
    </script>
  </body>
</html>
